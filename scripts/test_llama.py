from llama_cpp import Llama

# Path to your downloaded model
model_path = r"C:\llama\models\RoMistral-7b-Instruct.Q4_K_S.gguf"

# Load the model
llm = Llama(
    model_path=model_path,
    n_ctx=2048,
    n_threads=6,
    n_gpu_layers=30,  # Use 0 if CPU-only
    verbose=True
)

# Romanian legal prompt (Alpaca-style formatting)
prompt = """
### ÃŽntrebare:
Care este rolul CurÈ›ii ConstituÈ›ionale Ã®n Republica Moldova?

### RÄƒspuns:
"""

# Run the model
output = llm(prompt, max_tokens=256, stop=["###"])
print("\nðŸ§  RÄƒspuns generat:\n", output["choices"][0]["text"])
